{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIgvl27in38F"
   },
   "source": [
    "# Urban 8k Audio Classification Project\n",
    "\n",
    "## Dataset Description\n",
    "This dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. For a detailed description of the dataset and how it was compiled please refer to the paper.\n",
    "All excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.\n",
    "\n",
    "In addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided.\n",
    "\n",
    "**8732 audio files of urban sounds (see description above) in WAV format. The sampling rate, bit depth, and number of channels are the same as those of the original file uploaded to Freesound (and hence may vary from file to file).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd-xx4X5owj3"
   },
   "source": [
    "## Task Description\n",
    "The Dataset contains 10 classes, we will do a classification task on the audio files.\n",
    "```\n",
    "A numeric identifier of the sound class:\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQFYFyaKp-np"
   },
   "source": [
    "## How to get the Urban8K Dataset?\n",
    "Just go to [Urban8K Website](https://urbansounddataset.weebly.com/urbansound8k.html) and fill a simple form to download the dataset. Since the dataset is > 5GB in compressed form itself, it's better to copy the download link and directly !wget it to the colab and move to the drive for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQGh6AkPqkkI"
   },
   "source": [
    "### Code Block Below Downloads the Dataset and Unzips the tarball, this might take a while to download and unzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72xbqKLmnvnt",
    "outputId": "4dc5cb44-dfde-4a75-f3da-38c7a020a3cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '8hY5ER'\n"
     ]
    }
   ],
   "source": [
    "!wget https://goo.gl/8hY5ER\n",
    "!tar -xvf 8hY5ER -C ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VncIXbG7vUCx"
   },
   "source": [
    "* Mount Google Drive, Create a Folder named Urban8KZip in it.\n",
    "* Copy the unzipped urban8kzipped contents to the Urban8KZip Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhmlDU9cqymM",
    "outputId": "3105e031-e67c-4810-a954-b6601cca263c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15892\\1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wj8Kh6UGvHyp",
    "outputId": "c4a10644-4ab4-446c-953b-3fc68d5ad75d"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/drive/MyDrive/Urban8KZip\n",
    "!cp ./8hY5ER /content/drive/MyDrive/Urban8KZip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U79ZnSJ2vzhW"
   },
   "source": [
    "### In case of running notebook again,\n",
    "\n",
    "If you do run notebook again there is no reason to download dataset again and move to drive, just mount the drive and copy the content from the Urban8KZip folder to the colab.\n",
    "\n",
    "**Run this next cell if running the notebook again, Don't run the above cell and downloading cells(in starting), just mount the colab and run the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72jxfGyfxuvs",
    "outputId": "62d84772-3424-41eb-e1e0-785fe0510138"
   },
   "outputs": [],
   "source": [
    "!tar -xvf /content/drive/MyDrive/Urban8KZip/8hY5ER -C ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr7fTVtZx5BJ"
   },
   "source": [
    "## Data Analysis, Exploration and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3U24dpl0fbS"
   },
   "source": [
    "### Some Information About Dataset.\n",
    "This dataset has audio belonging to 10 classes, unlike images Audio data is different, it's similar in some senses but there are certain differences which are unique to audio data.\n",
    "\n",
    "Before even using audio file dataset, the first challenge in itself is that audio files are continous data and our computers can only store discrete data, so in what way we digitize it such that we can easily store.\n",
    "\n",
    "Apparently the solution is that we figure out a way that is discrete yet continous. Sounds misleading but what we do is  <b>instead of storing continous value, we just sample it at fixed durations like 0.2 second apart or less or more<b>. We just sample it and the rate at which it is sampled is known as **Sampling rate**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/08/23210623/sound.png\" width=\"350\" alt=\"my img\"/>\n",
    "  <figcaption> Image Courtesy Analytics Vidhya </figcaption>\n",
    "</figure>\n",
    "\n",
    "When we say we sample it, what are we actually doing?\n",
    "> We are recording the amplitude at that point.\n",
    "\n",
    "\n",
    "* So the audio data is nothing just the amplitude stored at regular intervals which mimic the original continous wave.\n",
    "\n",
    "* We take and store thousands of measurements per second. If we can take tons of measurements extremely quickly with enough possible amplitude values, we can effectively use these snapshots to reconstruct the resolution and complexity of an analog wave.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://www.izotope.com/en/learn/digital-audio-basics-sample-rate-and-bit-depth/_jcr_content/root/sectioncontainer_main/flexcontainer/flexcontainer_center/flexcontainer_center_top/image_1558274996.coreimg.82.1280.jpeg/1590799241393/reconstructing-the-original-signal.jpeg\n",
    "\" width= \"500\" alt=\"my img\"/>\n",
    "  <figcaption> Image Courtesy izotope </figcaption>\n",
    "</figure>\n",
    "\n",
    "* [Read more about the audio concepts involved such as bit depth and nyquist rate](https://www.izotope.com/en/learn/digital-audio-basics-sample-rate-and-bit-depth.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NFh8Lo81pyd",
    "outputId": "6189bd77-6063-439b-cc93-1dcc9d6de997"
   },
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEAtVGtNx2Rw"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd #Allows Audio files to be played directly in the notebook\n",
    "import librosa #library we will use to analyze sounds\n",
    "import librosa.display #library module which helps visualize the waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HyNIkzw3_IE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0WY8XZ44GPp"
   },
   "source": [
    "### Reading audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ME7rRFt4EXs",
    "outputId": "acfe7b07-22e8-4d6b-8ec5-2290a834ad78"
   },
   "outputs": [],
   "source": [
    "# Recursively find the files that end with .wav extension in the provided path\n",
    "audios = glob.glob(os.path.join(\"/content/UrbanSound8K/audio/*/*.wav\"), recursive=True)\n",
    "\n",
    "print(f\"Total Audio Files : {len(audios)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLTaWYi95ArL",
    "outputId": "5c87db77-effe-4595-a5af-f806e4cc2fb7"
   },
   "outputs": [],
   "source": [
    "audios[:5] #Contains path of all audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSAEMPa25P1i"
   },
   "source": [
    "### Hit the play icon below and listen to the sound, try to remember the sound and the plot generated do it several times and try to pick up  the pattern yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w4u7TiJ6Ig3"
   },
   "source": [
    "To read audio files, we will be using,\n",
    "\n",
    "```python\n",
    "data, sample_rate = librosa.load(audio_path)\n",
    "```\n",
    "\n",
    "librosa by default reads the audio file at sr(sampling rate) = 22050, if you want your custom sampling here, you can pass sr = your_rate in the librosa.load function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "wjxSAeNk4IYB",
    "outputId": "f2ce562c-db65-4925-84bd-f9f47e9a76af"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "play_audio = random.choice(audios)\n",
    "data, sample_rate = librosa.load(play_audio)\n",
    "librosa.display.waveshow(data)\n",
    "plt.title(\"Sound\")\n",
    "ipd.Audio(play_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPG5jKbA5ymL"
   },
   "source": [
    "#### **If you managed to pick up patterns, you can see it becomes a visual problem as well. Just a thought.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "RZPu2FLq5KF6",
    "outputId": "16c13198-8097-4471-ed6c-e628165653af"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "play_audio = random.choice(audios)\n",
    "data, sample_rate = librosa.load(play_audio, sr=100)\n",
    "librosa.display.waveshow(data)\n",
    "plt.title(\"Sound\")\n",
    "ipd.Audio(play_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgIoL8ri80hG"
   },
   "source": [
    "### Plotting same audio at different sampling rate\n",
    "\n",
    "Higher the sampling rate, higher the crisp of plot because less error while trying to recreate original audio with higher number of samples and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "02KJd1Tn5F34",
    "outputId": "096d7996-a369-4461-d0e7-150da6d72bc0"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(15,12))\n",
    "fig.suptitle(\"Sampling of Same Audio File With Different values\")\n",
    "axs = np.reshape(axs,-1)\n",
    "play_audio = random.choice(audios)\n",
    "srs = [100, 2000, 22050, 44100]\n",
    "for ax,sr in zip(axs,srs):\n",
    "  try:\n",
    "    data, sample_rate = librosa.load(play_audio, sr=sr)\n",
    "    librosa.display.waveshow(data, sr=sample_rate, ax=ax)\n",
    "    ax.set_title(f\"Sampling Rate {sr} Hz\")\n",
    "  except:\n",
    "    print(\"Run Again Some Unknown Error\")\n",
    "\n",
    "ipd.Audio(play_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6M8o1MC-_6w"
   },
   "source": [
    "### Reading Metadata for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cT3owspF9SeA",
    "outputId": "9f755a05-1945-41e2-ecfd-2609045f6437"
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"/content/UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p8CRXyU_RTU"
   },
   "source": [
    "#### Anatomy of Metadata csv\n",
    "* UrbanSound8K the main dataset directory contains two folders audio and metadata, the audio folder is where all the audio files are located, if you are running this in colab on right if you click on the UrbanSound8K Folder, you can see that it has the said two folders by clicking on it.\n",
    "* audio folder in itself contains 10 folders named fold1, fold2 ..... fold10. These folds contains audio files details of which are mentioned in above the csv or dataframe.\n",
    "\n",
    "* If we join \"UrbanSound8K/audio/\" + \"fold\" + fold_value(1/2/...10) + slice_file_name it becomes path for the audio file as well.(This logic will be implemented later, keep it in mind till then.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fAGvrRUApFa"
   },
   "source": [
    "#### Selecting random row for each class in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v27tHj6q_CGo"
   },
   "outputs": [],
   "source": [
    "unique_audios = metadata.groupby(['class']).apply(lambda sub_df : sub_df.sample()).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ieC6zB7oQhsS",
    "outputId": "f5f63e5f-57aa-47b7-8ac0-a6e4637e56a9"
   },
   "outputs": [],
   "source": [
    "unique_audios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9i9JTbdA1uA"
   },
   "source": [
    "#### Plotting selected rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "B9YV5D34AyDk",
    "outputId": "02eff44d-d60b-4e74-c5aa-bf4fe2ecc6c3"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2,figsize=(15,8),constrained_layout=True)\n",
    "axs = np.reshape(axs, -1)\n",
    "\n",
    "for (index,row),ax in zip(unique_audios.iterrows(),axs):\n",
    "  ax.set_title(row.values[-1])\n",
    "  data, sr = librosa.load(f\"/content/UrbanSound8K/audio/fold{row.values[-3]}/\" + row.values[0])\n",
    "  _ = librosa.display.waveshow(data, sr=sample_rate, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKn6NkSaBCkw"
   },
   "source": [
    "#### Let's see the number of instances for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "0oEHvC_QA91D",
    "outputId": "d2598dd3-6ca2-43da-a328-5096720d3236"
   },
   "outputs": [],
   "source": [
    "instance_counts = metadata['class'].value_counts()\n",
    "sns.set(rc={'figure.figsize':(11.7,4.14)})\n",
    "sns.barplot(x=instance_counts.values, y=instance_counts.index, orient='h');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX4kiUBUBGdK"
   },
   "source": [
    "* Apart from car_horn and gun_shot class all of the classes appear to be balanced, this might be a problem we will try to address later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KIR11qKDFuU"
   },
   "source": [
    "### **What is this mfcc and why are we using it?**\n",
    "\n",
    "> MFCCs are essentially like taking a Fourier Transform of the signal, however, MFCCs use Mel scaling to try to model the way that the human hearing audiotory system perceives sounds, rather than describe them on a purely frequency (Hz) basis. This means that the MFCC should represent the textural or timbre of the signal (the baby cry) as we might hear it (e.g. a 'piercing' cry or a 'discontent' cry).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "yqcFl3ccCsmH",
    "outputId": "b9fdfa7b-7ae7-4ae8-f869-e83affebeffc"
   },
   "outputs": [],
   "source": [
    "unique_audios_5 = unique_audios.head()\n",
    "fig, axs = plt.subplots(5,3,figsize=(15,8),constrained_layout=True)\n",
    "\n",
    "for (index,row),ax in zip(unique_audios_5.iterrows(),axs):\n",
    "  \n",
    "  file_path = f\"/content/UrbanSound8K/audio/fold{row.values[-3]}/\" + row.values[0]\n",
    "  ax[0].set_title(f\"Librosa : {row.values[-1]}\")\n",
    "  data, sr = librosa.load(file_path)\n",
    "  _ = librosa.display.waveshow(data, sr=sample_rate, ax=ax[0])\n",
    "\n",
    "  ax[1].set_title(f\"MFCC 40 : {row.values[-1]}\")\n",
    "  mfccs = librosa.feature.mfcc(y =data, n_mfcc= 40)\n",
    "  librosa.display.specshow(mfccs, x_axis='time', ax=ax[1])\n",
    "\n",
    "  ax[2].set_title(f\"MFCC 30 : {row.values[-1]}\")\n",
    "  mfccs = librosa.feature.mfcc(y =data, n_mfcc= 30)\n",
    "  librosa.display.specshow(mfccs, x_axis='time', ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTAJ2RHCDENA"
   },
   "outputs": [],
   "source": [
    "def extract_features(file_name, n_mfcc=40):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name) \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None \n",
    "    return mfccsscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "19698c8338fd4af5bdca4f3edc513ca2",
      "f95060cc5da64c00889551d263b8179a",
      "a74e4d35f76847acbeae6db6206dbdb1",
      "49ccc1d62ee541f0ad27f7da7debdecd",
      "6acf99a7e8fb4d36b2acc982c63f33c2",
      "85bebf7020b04c07a11ed88a5798d838",
      "96a9c39cb35a464c90ddc37149953a97",
      "3e93b0322f47458591f16595f8686f56",
      "4127ae2288b844a8a1443452837393b2",
      "20702ffb361e414ab7294bd3bc4b8a46",
      "56801e3a6a73490bb7ecf7ece175a6dd"
     ]
    },
    "id": "FR8_czDMDTsg",
    "outputId": "55265871-c733-4500-a04b-e0ee2c9172b6"
   },
   "outputs": [],
   "source": [
    "#Takes time\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "features = []\n",
    "for index,row in tqdm(metadata.iterrows()):\n",
    "    file_path = f\"/content/UrbanSound8K/audio/fold{row.values[-3]}/\" + row.values[0]\n",
    "    class_label = row.values[-1]\n",
    "    try:\n",
    "      data = extract_features(file_path)\n",
    "      features.append([data, class_label])\n",
    "    except:\n",
    "      pass\n",
    "      \n",
    "\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "NYluZKr1DZFN",
    "outputId": "527a9e41-71ca-4713-d18e-91d020aeb410"
   },
   "outputs": [],
   "source": [
    "featuresdf[featuresdf.feature.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd-eyspPEnGM"
   },
   "outputs": [],
   "source": [
    "featuresdf['class_label'] = pd.Categorical(featuresdf['class_label'])\n",
    "featuresdf['class_category'] = featuresdf['class_label'].cat.codes\n",
    "\n",
    "featuresdf['feature_len'] = featuresdf['feature'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "MAoXdISXEnzB",
    "outputId": "2be288d2-d5a5-4db2-ba22-17bbc140ed4c"
   },
   "outputs": [],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noDynvVwOZOR"
   },
   "outputs": [],
   "source": [
    "featuresdf.to_json(\"features.json\", orient='records')\n",
    "!cp ./features.json /content/drive/MyDrive/Urban8KZip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHB9werQOfYU"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/Urban8KZip/features.json ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHbX3aQsOfv9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "featuresdf = pd.read_json(\"features.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DV9BFnhCOmw3",
    "outputId": "66b851e2-96b5-4415-c401-8140962bd7ff"
   },
   "outputs": [],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRJQnzkyN0BW",
    "outputId": "22dc010a-c00c-4a36-a931-76c24af4213d"
   },
   "outputs": [],
   "source": [
    "featuresdf.class_category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4MW_FZ2Orte"
   },
   "source": [
    "## Model Building & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J41zRWoQj6q"
   },
   "source": [
    "### Converting Data to Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcfiYEsDOoTZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTirACt7O_fB"
   },
   "source": [
    "The Data is not in the desired format, we will do the follow the steps to convert it in our required format.\n",
    "1. Split Dataset using sklearn\n",
    "2. Convert the split data into torch tensors\n",
    "3. Convert the torch tensors into Tensor Datasets.\n",
    "4. Created DataLoaders from these Tensor Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMTHL81APh4-"
   },
   "source": [
    "#### 1. Splitting Dataset using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSdjWxt-Ovn5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X = featuresdf.feature\n",
    "y = featuresdf.class_category\n",
    "x_train, x_test, y_train, y_test = train_test_split(list(X.values), list(y.values), test_size=0.2, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVZQUBvDOxCv"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train_base = x_train\n",
    "x_test_base = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdsdx6obPoIb"
   },
   "source": [
    "#### 2. Converting Data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqD1AODbOzgA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array(x_train_base)\n",
    "x_test = np.array(x_test_base)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrgNJ3oYO4pu"
   },
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpNBBTO5O7pG",
    "outputId": "e8be1ccc-d0f4-4256-947e-ee3fae068966"
   },
   "outputs": [],
   "source": [
    "print(x_train_tensor.type(),y_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFz8cCf4Pq2I"
   },
   "source": [
    "#### 3,4 Converting into TensorDataset and feeding it to DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRc4FPI1O9D9"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_tensor,y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor,y_test_tensor)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=64*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTxm-7YtQgoP"
   },
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJjD5aTmQcIG"
   },
   "source": [
    "#### Fixing Input and Output Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRiD6YhWQPuu"
   },
   "outputs": [],
   "source": [
    "input_size = 40\n",
    "output_size = featuresdf['class_category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7TU8s_WFijX",
    "outputId": "b4d50611-7275-4031-e1c9-512c4e3bfb69"
   },
   "outputs": [],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap3R5cpoXgnZ"
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0K1aA2F0BqZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "class UrbanSoundBase(nn.Module):\n",
    "\n",
    "  def training_step(self, batch_images):\n",
    "    images,labels = batch_images\n",
    "    outputs = self(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    acc = self.accuracy(outputs, labels)\n",
    "    return {'loss' : loss, 'acc' : acc}\n",
    "\n",
    "  def accuracy(self,outputs, labels, logits=True):\n",
    "    output_softmaxed = F.softmax(outputs, dim=1)\n",
    "    vals,predictions, = torch.max(output_softmaxed, dim=1)\n",
    "    assert predictions.shape == labels.shape\n",
    "    return torch.tensor(torch.sum(predictions == labels).item()/outputs.size(0))\n",
    "\n",
    "  def validation_step(self, batch_images, device):\n",
    "    images,labels = batch_images\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = self(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    acc = self.accuracy(outputs, labels)\n",
    "\n",
    "    return {'val_loss' : loss, 'val_acc' : acc}\n",
    "\n",
    "  def epoch_end(self, history):\n",
    "    loss = torch.stack([batch['val_loss'] for batch in history]).mean().item()\n",
    "    accuracy = torch.stack([batch['val_acc'] for batch in history]).mean().item()\n",
    "\n",
    "    return {'val_loss': loss, 'val_acc' : accuracy}\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def evaluate_validation(self, valid_dataloader, device):\n",
    "    return [self.validation_step(image_label_batch, device) for image_label_batch in valid_dataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfoJiFup0ITk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "class NNModel_3Layers(UrbanSoundBase):\n",
    "    \"\"\"\n",
    "    Layer Size : 128, 256, 10\n",
    "    Drop Outs : True, True, True\n",
    "    Drop Values : 0.2, 0.3, 0.2\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.block1(xb)\n",
    "        out = self.block2(out)\n",
    "        return self.block3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pg1ltQf0UM8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "class NNModel_4Layers(UrbanSoundBase):\n",
    "    \"\"\"\n",
    "    Layer Size : 128, 256, 256, 10\n",
    "    Drop Outs : True, True, True, True\n",
    "    Drop Values : 0.2, 0.3, 0.3, 0.1\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))          \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.block1(xb)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        return self.block4(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C55J8fkm0VHI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "class NNModel_5Layers(UrbanSoundBase):\n",
    "    \"\"\"\n",
    "    Layer Size : 128, 128, 256, 256, 10\n",
    "    Drop Outs : True, True, True, True, True\n",
    "    Drop Values : 0.2, 0.3, 0.3, 0.2, 0.1\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)) \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.block1(xb)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        return self.block5(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_JPxDDU0VWs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "class NNModel_6Layers(UrbanSoundBase):\n",
    "    \"\"\"\n",
    "    Layer Size : 128, 256, 512, 512, 256, 10\n",
    "    Drop Outs : True, True, False, True, True\n",
    "    Drop Values : 0.2, 0.3, _, 0.3, 0.1\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))   \n",
    "        \n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)) \n",
    "        \n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(256, output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)) \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.block1(xb)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.block5(out)\n",
    "        return self.block6(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tsLdoBKQ7o6"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTLfxplLQ9FR"
   },
   "source": [
    "#### Accuracy and Loss Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aA3biKIoQ2L4",
    "outputId": "09a62e14-818d-46ed-cd75-55aceef2bd36"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McRTnGBDRAEI",
    "outputId": "a87ee702-39ab-4d31-8950-4bbc0aee5638"
   },
   "outputs": [],
   "source": [
    "model_4_layers.eval()\n",
    "valid_history = model_4_layers.evaluate_validation(test_dl, device)\n",
    "acc = torch.stack([valid['val_acc'] for valid in valid_history]).mean().item()\n",
    "loss = torch.stack([valid['val_loss'] for valid in valid_history]).mean().item()\n",
    "\n",
    "print(f\"Accuracy : {acc:.3f}, Loss : {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp6pMVLPXkFc"
   },
   "source": [
    "#### Fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ra_YZAoRBjz"
   },
   "outputs": [],
   "source": [
    "def fit(model,train_dataloader, valid_dataloader, n_epochs,lr = 0.001, optim = torch.optim.Adam, device = device):\n",
    "  model_history = {'train_loss' : [], 'valid_loss' : [], 'train_acc' : [], 'valid_acc' : []}\n",
    "  optimizer = optim(model.parameters(), lr)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "\n",
    "    for image_label_batch in train_dataloader:\n",
    "      image, label = image_label_batch[0].to(device), image_label_batch[1].to(device)\n",
    "      loss_acc = model.training_step((image, label))\n",
    "      loss = loss_acc['loss']\n",
    "      acc = loss_acc['acc']\n",
    "      loss.backward()\n",
    "      train_loss.append(loss)\n",
    "      train_accuracy.append(acc)\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_history = model.evaluate_validation(valid_dataloader, device)\n",
    "        result = model.epoch_end(val_history)\n",
    "\n",
    "    model_history['train_loss'].append(torch.stack(train_loss).mean().item())\n",
    "    model_history['valid_loss'].append(result['val_loss'])\n",
    "    model_history['train_acc'].append(torch.stack(train_accuracy).mean().item())\n",
    "    model_history['valid_acc'].append(result['val_acc'])\n",
    "    print(f\"Epoch : {epoch}, Train Loss : {model_history['train_loss'][-1]:.2f}, Train Accuracy : {model_history['train_acc'][-1]:.2f}, Validation Loss : {result['val_loss']:.2f}, Validation Accuracy : {result['val_acc']:.2f}\")\n",
    "\n",
    "\n",
    "  return model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaSVuwuheJP9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdPpgcPxXmuV"
   },
   "source": [
    "#### Test Case : n_epoch = 300, lr=0.001, SGD, final_acc = 18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUxxk0esTQGz",
    "outputId": "e504d5e6-8680-422c-c99d-3fcc0ec1c57c"
   },
   "outputs": [],
   "source": [
    "history = fit(model_4_layers, train_dl, test_dl, 300, 0.001, optim = torch.optim.SGD, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "hBoNi5zPd2EI",
    "outputId": "abf6b769-6c97-4a83-d235-1dcbde3eb847"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_4_layers_simple : 300 Epochs, 0.001 SGD\", fontsize=16)\n",
    "ax1.plot(history['train_acc'], label='train accuracy')\n",
    "ax1.plot(history['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history['train_loss'], label='train loss')\n",
    "ax2.plot(history['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReiQru4EXyo6"
   },
   "source": [
    "#### Test Case : n_epoch = 1000, lr=0.001, SGD, final_acc = 74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnlBWOONTWrp",
    "outputId": "9223db26-bdc2-4868-d3cd-8afb24481590"
   },
   "outputs": [],
   "source": [
    "model_4_layers = NNModel_4Layers(input_size, output_size)\n",
    "history2 = fit(model_4_layers, train_dl, test_dl, 1000, 0.001, optim=torch.optim.SGD, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "JABXyaMXexpw",
    "outputId": "266bc90a-a12c-4eb4-fc69-323b64eb257a"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_4_layers_simple : 1000 Epochs, 0.001 SGD\", fontsize=16)\n",
    "ax1.plot(history2['train_acc'], label='train accuracy')\n",
    "ax1.plot(history2['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history2['train_loss'], label='train loss')\n",
    "ax2.plot(history2['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtGVLYNQX1Uh"
   },
   "source": [
    "#### Test Case : n_epoch = 500, lr=0.01, SGD, final_acc = 73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_nGSThyTjg5",
    "outputId": "e76cf2ac-448d-4f13-e6da-28479c4cd195"
   },
   "outputs": [],
   "source": [
    "model_4_layers = NNModel_4Layers(input_size, output_size)\n",
    "history3 = fit(model_4_layers, train_dl, test_dl, 500, 0.01, optim=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "1Hu8r5bNe6Tz",
    "outputId": "2ab7c49d-40a3-4a60-a094-e1790b6bb29a"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_4_layers_simple : 500 Epochs, 0.01 SGD\", fontsize=16)\n",
    "ax1.plot(history3['train_acc'], label='train accuracy')\n",
    "ax1.plot(history3['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history3['train_loss'], label='train loss')\n",
    "ax2.plot(history3['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezG4y2XvfajB",
    "outputId": "69cb6fca-cf86-4f39-b6a1-bfca676cc11a"
   },
   "outputs": [],
   "source": [
    "model_5_layers = NNModel_5Layers(input_size, output_size)\n",
    "history4 = fit(model_5_layers, train_dl, test_dl, 500, 0.01, optim=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "NrE1eCoTiEhO",
    "outputId": "5a044259-a66d-421c-dabd-a182f5142d25"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_5_layers_simple : 500 Epochs, 0.01 SGD\", fontsize=16)\n",
    "ax1.plot(history4['train_acc'], label='train accuracy')\n",
    "ax1.plot(history4['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history4['train_loss'], label='train loss')\n",
    "ax2.plot(history4['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjBV1LE6ke0d",
    "outputId": "b5ad85e4-612c-4ec2-95b3-d63f77b67f94"
   },
   "outputs": [],
   "source": [
    "model_3_layers = NNModel_3Layers(input_size, output_size)\n",
    "history5 = fit(model_3_layers, train_dl, test_dl, 500, 0.001, optim=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "WtrXzv1ynPmF",
    "outputId": "d980023c-303b-4228-aec0-ce7b0ce1a9fd"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_3_layers_simple : 500 Epochs, 0.001 SGD\", fontsize=16)\n",
    "ax1.plot(history5['train_acc'], label='train accuracy')\n",
    "ax1.plot(history5['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history5['train_loss'], label='train loss')\n",
    "ax2.plot(history5['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1U7X81znjo1",
    "outputId": "204f7ecf-bf8d-455d-cff4-fc0e7832a915"
   },
   "outputs": [],
   "source": [
    "model_3_layers = NNModel_3Layers(input_size, output_size)\n",
    "history6 = fit(model_3_layers, train_dl, test_dl, 500, 0.001, optim=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "sOyzIhbeoMsQ",
    "outputId": "cacf8966-68d0-4c4c-802f-fc474d6fc997"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_3_layers_simple : 500 Epochs, 0.001 Adam\", fontsize=16)\n",
    "ax1.plot(history6['train_acc'], label='train accuracy')\n",
    "ax1.plot(history6['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history6['train_loss'], label='train loss')\n",
    "ax2.plot(history6['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VXrBqHOpHBb",
    "outputId": "f16bbcb4-0a95-4c98-b79d-5e4f1a498874"
   },
   "outputs": [],
   "source": [
    "model_3_layers = NNModel_3Layers(input_size, output_size)\n",
    "history7 = fit(model_3_layers, train_dl, test_dl, 500, 3e-4, optim=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "xtUmDOm5pNB_",
    "outputId": "d44bce82-0587-4f4f-8d1c-c6346b487f7f"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_3_layers_simple : 500 Epochs, 3e-4 Adam\", fontsize=16)\n",
    "ax1.plot(history7['train_acc'], label='train accuracy')\n",
    "ax1.plot(history7['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history7['train_loss'], label='train loss')\n",
    "ax2.plot(history7['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kgPPOuzqvbO",
    "outputId": "f14437f2-c91f-4990-c2bb-5658a17df686"
   },
   "outputs": [],
   "source": [
    "model_6_layers = NNModel_6Layers(input_size, output_size)\n",
    "history8 = fit(model_6_layers, train_dl, test_dl, 500, 0.001, optim=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "F-9iSpRbvHkP",
    "outputId": "57162f4c-c28f-4c7e-cef5-8fbedecdf129"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_3_layers_simple : 500 Epochs, 3e-4 Adam\", fontsize=16)\n",
    "ax1.plot(history8['train_acc'], label='train accuracy')\n",
    "ax1.plot(history8['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history8['train_loss'], label='train loss')\n",
    "ax2.plot(history8['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laWJtrelzcW-",
    "outputId": "f579b742-af5c-44a5-8ec8-a7cfd5e091c8"
   },
   "outputs": [],
   "source": [
    "model_5_layers = NNModel_5Layers(input_size, output_size)\n",
    "history4 = fit(model_5_layers, train_dl, test_dl, 500, 0.001, optim=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "10LZivWKzaEQ",
    "outputId": "f9993973-e3b8-4ff4-999b-b7c7c1e939c6"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "fig.suptitle(f\"model_nn_5_layers_simple : 500 Epochs, 0.001 SGD\", fontsize=16)\n",
    "ax1.plot(history4['train_acc'], label='train accuracy')\n",
    "ax1.plot(history4['valid_acc'], label='valid accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(history4['train_loss'], label='train loss')\n",
    "ax2.plot(history4['valid_loss'], label='valid loss')\n",
    "ax2.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnVI-Z9ynZmM"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WIXSn6n4oJMK",
    "outputId": "8acdac1e-58e2-4127-c361-5138a89b93e3"
   },
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "y-4KoG2LZEF2",
    "outputId": "a34b09cc-b374-4a15-f768-b7a09dc992a6"
   },
   "outputs": [],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOaQj8GZoX-K"
   },
   "outputs": [],
   "source": [
    "meta_feature = pd.concat([metadata, featuresdf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "x1ukmZOsphKl",
    "outputId": "0daefc83-c710-480c-b9ac-a8032ed8dabb"
   },
   "outputs": [],
   "source": [
    "meta_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8QNWCLFnkhd"
   },
   "source": [
    "**Finally using model_5_layers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf63wl-hq8mf"
   },
   "source": [
    "Creating Noise to Index Mapping and Vice-Versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "MVUCuDBVq5XE",
    "outputId": "68463acb-1b39-48f4-ed37-6825aecd4745"
   },
   "outputs": [],
   "source": [
    "meta_feature.loc[:,[\"class_label\", \"class_category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYunMEK2p9IO"
   },
   "outputs": [],
   "source": [
    "#For converting the numerical codes of the labels into categorical values for readability\n",
    "noise_2_idx = dict(set(meta_feature[['class_label', 'class_category']].to_records(index=False).tolist()))\n",
    "idx_2_noise = { idx : noise for noise,idx in noise_2_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUAo4nLEle2U"
   },
   "outputs": [],
   "source": [
    "random_samples = meta_feature.sample(5)\n",
    "msfcc_feature, labels = random_samples.feature.values.tolist(), random_samples.class_category.values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "EZI68MYcI5rN",
    "outputId": "9f197b26-3149-4013-fc24-c94285c57b63"
   },
   "outputs": [],
   "source": [
    "random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaI72KUYpxgb",
    "outputId": "b39e5d7b-8c07-405d-e5d4-5d7ae47bfb87"
   },
   "outputs": [],
   "source": [
    "msfcc_feature = torch.tensor(msfcc_feature)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(msfcc_feature.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEhfTiezr89M",
    "outputId": "780aa687-319b-4375-9e65-300a669f0077"
   },
   "outputs": [],
   "source": [
    "model_5_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YDQCplunJcd"
   },
   "outputs": [],
   "source": [
    "outputs = model_5_layers(msfcc_feature)\n",
    "\n",
    "output_softmaxed = F.softmax(outputs, dim=1)\n",
    "_, preds = torch.max(output_softmaxed, dim=1)\n",
    "\n",
    "preds = preds.numpy().tolist()\n",
    "preds_labels = [idx_2_noise[pred] for pred in preds]\n",
    "\n",
    "\n",
    "labels = labels.numpy().tolist()\n",
    "labels_names = [idx_2_noise[label] for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8MJiyeFtcFw"
   },
   "source": [
    "## Final Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUeAGc6BtS74",
    "outputId": "4e8ad1bd-4aa4-4e6a-fefb-3e004d1d1c12"
   },
   "outputs": [],
   "source": [
    "random_samples.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "TdFcR59NrXnB",
    "outputId": "adb43e96-3b17-40a1-c0d9-5e18df1b86b7"
   },
   "outputs": [],
   "source": [
    "audio_path = f\"/content/UrbanSound8K/audio/fold{random_samples.iloc[0].values[5]}/\" + random_samples.iloc[0].values[0]\n",
    "data, sr = librosa.load(audio_path)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "_ = librosa.display.waveshow(data)\n",
    "\n",
    "plt.title(f\"Original Sound : {labels_names[0]}, Predicted : {preds_labels[0]}\", fontsize=16)\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "n9Fn2__isWc7",
    "outputId": "d7a3266a-5c70-4202-a6f3-cab61ef615c2"
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "audio_path = f\"/content/UrbanSound8K/audio/fold{random_samples.iloc[idx].values[5]}/\" + random_samples.iloc[idx].values[0]\n",
    "data, sr = librosa.load(audio_path)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "_ = librosa.display.waveshow(data)\n",
    "\n",
    "plt.title(f\"Original Sound : {labels_names[idx]}, Predicted : {preds_labels[idx]}\", fontsize=16)\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "1MZ1QAcJti1J",
    "outputId": "793aab06-46eb-4c3a-9870-acc7c2df274a"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "audio_path = f\"/content/UrbanSound8K/audio/fold{random_samples.iloc[idx].values[5]}/\" + random_samples.iloc[idx].values[0]\n",
    "data, sr = librosa.load(audio_path)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "_ = librosa.display.waveshow(data)\n",
    "\n",
    "plt.title(f\"Original Sound : {labels_names[idx]}, Predicted : {preds_labels[idx]}\", fontsize=16)\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "rhD38cOctlxf",
    "outputId": "20d3eff6-7981-401d-ddbb-eecf940cc376"
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "audio_path = f\"/content/UrbanSound8K/audio/fold{random_samples.iloc[idx].values[5]}/\" + random_samples.iloc[idx].values[0]\n",
    "data, sr = librosa.load(audio_path)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "_ = librosa.display.waveshow(data)\n",
    "\n",
    "plt.title(f\"Original Sound : {labels_names[idx]}, Predicted : {preds_labels[idx]}\", fontsize=16)\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "aJNfHJM0tmpJ",
    "outputId": "7792d169-1629-4f13-8057-c631eff4672c"
   },
   "outputs": [],
   "source": [
    "idx = 4\n",
    "audio_path = f\"/content/UrbanSound8K/audio/fold{random_samples.iloc[idx].values[5]}/\" + random_samples.iloc[idx].values[0]\n",
    "data, sr = librosa.load(audio_path)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "_ = librosa.display.waveshow(data)\n",
    "\n",
    "plt.title(f\"Original Sound : {labels_names[idx]}, Predicted : {preds_labels[idx]}\", fontsize=16)\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aVc8BHXpts-"
   },
   "source": [
    "# Future Scope and Other Methods to Try:\n",
    "* MFCC is not the only feature that can be utilized in audio analysis, beside that you can try Volume, Energy, Pitch, Zero Crossing Rate, Spectral Centroid etc as additional features along with it.\n",
    "\n",
    "* Instead of extracting features via MFCC, feature reduction technique such as t-SNE or PCA can be used as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GE6tQkQ134zV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19698c8338fd4af5bdca4f3edc513ca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f95060cc5da64c00889551d263b8179a",
       "IPY_MODEL_a74e4d35f76847acbeae6db6206dbdb1",
       "IPY_MODEL_49ccc1d62ee541f0ad27f7da7debdecd"
      ],
      "layout": "IPY_MODEL_6acf99a7e8fb4d36b2acc982c63f33c2"
     }
    },
    "20702ffb361e414ab7294bd3bc4b8a46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e93b0322f47458591f16595f8686f56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "4127ae2288b844a8a1443452837393b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49ccc1d62ee541f0ad27f7da7debdecd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20702ffb361e414ab7294bd3bc4b8a46",
      "placeholder": "​",
      "style": "IPY_MODEL_56801e3a6a73490bb7ecf7ece175a6dd",
      "value": " 8732/? [06:14&lt;00:00, 15.28it/s]"
     }
    },
    "56801e3a6a73490bb7ecf7ece175a6dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6acf99a7e8fb4d36b2acc982c63f33c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85bebf7020b04c07a11ed88a5798d838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96a9c39cb35a464c90ddc37149953a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74e4d35f76847acbeae6db6206dbdb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e93b0322f47458591f16595f8686f56",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4127ae2288b844a8a1443452837393b2",
      "value": 1
     }
    },
    "f95060cc5da64c00889551d263b8179a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85bebf7020b04c07a11ed88a5798d838",
      "placeholder": "​",
      "style": "IPY_MODEL_96a9c39cb35a464c90ddc37149953a97",
      "value": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
